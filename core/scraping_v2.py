"""
HOTARU SMART SCRAPER V2 (core/scraping_v2.py)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Moteur : Crawl4AI (Playwright async) â†’ x5 Ã  x10 plus rapide que V1
Logique mÃ©tier : 100% prÃ©servÃ©e (JSON-LD, patterns, titles, normalization)

NouveautÃ©s vs V1 :
  âœ… Crawl parallÃ¨le async (20-50 pages simultanÃ©es)
  âœ… Markdown LLM-ready natif (fit_markdown sans nav/footer/ads)
  âœ… Extraction structurÃ©e CSS/XPath sans LLM
  âœ… Cache persistant entre sessions
  âœ… Anti-dÃ©tection natif (Playwright + stealth)
  âœ… Crash recovery (resume_state sur long crawls)
  âœ… Interface identique Ã  V1 (mÃªme run_analysis, mÃªme dict rÃ©sultat)

CompatibilitÃ© :
  - Drop-in replacement de SmartScraper
  - MÃªme clÃ©s de sortie : url, title, links, description, h1,
    response_time, html_content, has_structured_data, json_ld,
    h2_count, lists_count, markdown (NOUVEAU)
"""

import asyncio
import json
import time
import re
from urllib.parse import urlparse, urljoin, urlunparse
from typing import Optional, List, Dict, Any, Callable

# â”€â”€ Crawl4AI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy

# â”€â”€ Parsing classique (pour JSON-LD soup + titres) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from bs4 import BeautifulSoup
import requests

# Constantes partagÃ©es (Ã©vite listes recrÃ©Ã©es Ã  chaque instance)
EXCLUDE_PATTERNS = (
    ".pdf", ".jpg", ".jpeg", ".png", ".gif", ".zip",
    ".doc", ".docx", "tel:", "mailto:", "javascript:", "void(0)",
)
PAGE_TIMEOUT_MS = 30000
MAX_QUEUE_LINKS = 5000


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  HOTARU SCRAPER V2
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class HotaruScraperV2:
    """
    Remplace SmartScraper avec Crawl4AI comme moteur.
    Interface 100% compatible avec V1.
    """

    def __init__(
        self,
        start_urls,
        max_urls: int = 500,
        use_selenium: bool = False,       # IgnorÃ© (Playwright natif)
        selenium_mode: str = None,        # IgnorÃ© (Playwright natif)
        log_callback: Callable = None,
        proxy: Optional[str] = None,
        cache: bool = True,               # NOUVEAU : cache entre sessions
        concurrency: int = 10,            # NOUVEAU : pages en parallÃ¨le
        extra_domains: Optional[List[str]] = None,  # Domaines rattachÃ©s (site multi-domaines)
    ):
        # â”€â”€ Normalisation des URLs d'entrÃ©e â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if isinstance(start_urls, str):
            start_urls = [start_urls]

        self.domain = urlparse(start_urls[0]).netloc.lower()
        # Accepter www.example.com et example.com comme mÃªme domaine (Ã©vite 0 lien dÃ©couvert)
        def _netloc_variants(netloc: str) -> set:
            n = netloc.lower()
            b = n.removeprefix("www.") if n.startswith("www.") else n
            return {n, b, f"www.{b}"}
        self._domain_set = _netloc_variants(self.domain)
        # Domaines rattachÃ©s (site sous deux domaines diffÃ©rents)
        for raw in (extra_domains or []):
            raw = (raw or "").strip()
            if not raw:
                continue
            if "://" not in raw:
                raw = "https://" + raw
            netloc = urlparse(raw).netloc.lower()
            if netloc:
                self._domain_set |= _netloc_variants(netloc)
        self.start_urls = [self.normalize_url(url) for url in start_urls]
        self.base_url = self.start_urls[0]
        self.max_urls = max_urls
        self.log_callback = log_callback
        self.proxy = proxy
        self.cache = cache
        self.concurrency = concurrency

        # RÃ©sultats
        self.results: List[Dict] = []
        self.visited: set = set()

        # VÃ©rification : les URLs de dÃ©part sont du mÃªme domaine (premiÃ¨re URL)
        for url in self.start_urls:
            if urlparse(url).netloc.lower() != self.domain:
                raise ValueError(
                    f"Toutes les URLs de dÃ©part doivent Ãªtre du mÃªme domaine. "
                    f"TrouvÃ©: {urlparse(url).netloc} au lieu de {self.domain}"
                )

        # Filtres (identiques Ã  V1, rÃ©fÃ©rence constante)
        self.exclude_patterns = list(EXCLUDE_PATTERNS)

        # Compteurs (compatibles V1 + nouveaux)
        self.stats = {
            "pages_crawled": 0,
            "pages_skipped": 0,
            "links_discovered": 0,
            "links_filtered": 0,
            "links_duplicate": 0,
            "errors": 0,
            "start_urls_count": len(self.start_urls),
            "cache_hits": 0,
            "engine": "Crawl4AI v0.8 (Playwright)",
            "concurrency": concurrency,
            "proxy_used": proxy or "Aucun",
        }

        self.filtered_log = []
        self.duplicate_log = []

        self._log("ğŸš€ HotaruScraperV2 initialisÃ©")
        self._log(f"   Domaine : {self.domain}")
        if extra_domains:
            self._log(f"   Domaines rattachÃ©s : {len(extra_domains)}")
        self._log(f"   Max URLs : {max_urls}")
        self._log(f"   Concurrence : {concurrency} pages en parallÃ¨le")
        self._log(f"   Cache : {'activÃ©' if cache else 'dÃ©sactivÃ©'}")
        self._log(f"   Proxy : {proxy or 'Aucun'}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  UTILITAIRES (identiques Ã  V1)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def normalize_url(self, url: str) -> str:
        """Normalise une URL pour Ã©viter les doublons."""
        url = url.split("#")[0].split("?")[0]
        parsed = urlparse(url)
        path = parsed.path.rstrip("/") or "/"
        path = path.lower()
        return urlunparse((
            parsed.scheme.lower(),
            parsed.netloc.lower(),
            path, "", "", ""
        ))

    def _log(self, message: str):
        """Log visible dans Streamlit ET console."""
        print(message)
        if self.log_callback:
            self.log_callback(message)

    def is_valid_url(self, url: str) -> bool:
        """Filtre les URLs non pertinentes."""
        for pattern in self.exclude_patterns:
            if pattern in url.lower():
                self.filtered_log.append((url, pattern))
                return False
        return True

    def clean_title(self, title: str, h1: str, url: str) -> str:
        """Nettoie le titre (identique Ã  V1)."""
        domain_name = urlparse(url).netloc.split(".")[0].lower()

        def is_useful(text):
            if not text or len(text) < 3:
                return False
            text_clean = text.lower().replace(" ", "").replace("-", "")
            if domain_name in text_clean and len(text_clean) < len(domain_name) + 5:
                return False
            return True

        text = h1 if (h1 and len(h1) > 10) else (title if (title and len(title) > 5) else "")

        if text:
            separators = [" - ", " | ", " : ", " â€” ", " â€“ ", " Â· "]
            for sep in separators:
                if sep in text:
                    parts = [p.strip() for p in text.split(sep)]
                    useful_parts = [p for p in parts if is_useful(p)]
                    if useful_parts:
                        text = max(useful_parts, key=len)
                    break

        if not is_useful(text):
            path = urlparse(url).path
            segments = [s for s in path.split("/") if s and s not in ["fr", "en", "de", "es", "www"]]
            text = segments[-1].replace("-", " ").replace("_", " ").title() if segments else "Accueil"

        text = text.strip()
        return text[:40] + ".." if len(text) > 40 else text

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  JSON-LD (logique mÃ©tier V1 prÃ©servÃ©e + amÃ©liorÃ©e)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _extract_jsonld_from_soup(self, soup: BeautifulSoup) -> List[Dict]:
        """Extraction JSON-LD depuis BeautifulSoup (identique V1)."""
        out = []
        for script in soup.find_all("script"):
            t = (script.get("type") or "").lower()
            if "ld+json" not in t:
                continue
            try:
                raw = script.string or script.get_text(strip=True) or ""
                if not raw.strip():
                    continue
                out.append(json.loads(raw))
            except (json.JSONDecodeError, TypeError):
                continue
        return out

    def _extract_jsonld_from_html(self, html: str) -> List[Dict]:
        """
        Extraction JSON-LD robuste depuis HTML brut via regex.
        ComplÃ©ment Ã  soup pour les cas edge (scripts dynamiques injectÃ©s).
        """
        pattern = r'<script[^>]+type=["\']application/ld\+json["\'][^>]*>(.*?)</script>'
        matches = re.findall(pattern, html, re.IGNORECASE | re.DOTALL)
        out = []
        for raw in matches:
            raw = raw.strip()
            if not raw:
                continue
            try:
                out.append(json.loads(raw))
            except (json.JSONDecodeError, TypeError):
                continue
        return out

    def _merge_jsonld_no_duplicates(self, list_a: List, list_b: List = None) -> List:
        """Fusionne deux listes JSON-LD sans doublons (identique V1)."""
        list_b = list_b or []
        seen = set()
        merged = []
        for block in list_a + list_b:
            if block is None:
                continue
            try:
                canonical = json.dumps(block, sort_keys=True)
                if canonical in seen:
                    continue
                seen.add(canonical)
                merged.append(block)
            except (TypeError, ValueError):
                continue
        return merged

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  CONSTRUCTION RÃ‰SULTAT (compatible V1 + champs nouveaux)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _build_page_result(
        self,
        url: str,
        crawl_result,          # CrawlResult de Crawl4AI
    ) -> Optional[Dict]:
        """
        Construit le dict rÃ©sultat standard depuis un CrawlResult Crawl4AI.
        ClÃ©s identiques Ã  V1 + 'markdown' et 'fit_markdown' en bonus.
        """
        try:
            # DEBUG: Log au dÃ©but pour tracer l'exÃ©cution
            self._log(f"  ğŸ” Traitement: {url}")
            html_content = crawl_result.html or ""
            soup = BeautifulSoup(html_content, "html.parser")

            # â”€â”€ Titre & H1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            raw_title = soup.title.string.strip() if soup.title else ""
            h1_tag = soup.find("h1")
            h1 = h1_tag.get_text().strip() if h1_tag else ""
            final_title = self.clean_title(raw_title, h1, url)

            # â”€â”€ Meta description â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            meta_desc = ""
            meta_tag = soup.find("meta", attrs={"name": "description"})
            if meta_tag and meta_tag.get("content"):
                meta_desc = meta_tag["content"].strip()

            # â”€â”€ Liens internes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            links = []
            normalized_current = self.normalize_url(url)

            # *** FUSION COMPLÃˆTE de tous les fallbacks (au lieu de if/elif) ***
            # Cela garantit qu'on collecte ALL liens, d'TOUTES les sources
            raw_links_set = set()  # Ã‰vite les doublons entre sources

            # 1. Liens extraits par Crawl4AI (prioritÃ© haute)
            if getattr(crawl_result, "links", None):
                internal = (crawl_result.links or {}).get("internal", []) or []
                for link_obj in internal:
                    if isinstance(link_obj, dict):
                        href = link_obj.get("href", "")
                    else:
                        href = str(link_obj)
                    if href and href.startswith(("http", "/")):
                        raw_links_set.add(href)
                        self._log(f"    [Crawl4AI] Lien trouvÃ©: {href[:60]}")

            # 2. Liens <a href> depuis soup (TOUJOURS exÃ©cuter, mÃªme si Crawl4AI trouve des liens)
            soup_links = [a["href"] for a in soup.find_all("a", href=True)]
            if soup_links:
                for href in soup_links:
                    if href and href.startswith(("http", "/")):
                        raw_links_set.add(href)
                self._log(f"    [Soup <a>] {len(soup_links)} lien(s) trouvÃ©(s)")

            # 3. Liens data-href (SPA / frameworks modernes)
            data_href_links = []
            for tag in soup.find_all(attrs={"data-href": True}):
                h = tag.get("data-href", "").strip()
                if h and (h.startswith(("http", "/")) or not h.startswith("#")):
                    data_href_links.append(h)
                    raw_links_set.add(h)
            if data_href_links:
                self._log(f"    [data-href] {len(data_href_links)} lien(s) trouvÃ©(s)")

            # 4. Liens retournÃ©s par le JS injectÃ© (DOM rendu aprÃ¨s scroll)
            js_res = getattr(crawl_result, "js_execution_result", None)
            if js_res is not None:
                js_links = []
                if isinstance(js_res, list):
                    js_links = [
                        u for u in js_res
                        if isinstance(u, str) and u.startswith(("http", "/"))
                    ]
                elif isinstance(js_res, dict):
                    for v in js_res.values():
                        if isinstance(v, list):
                            js_links = [
                                u for u in v
                                if isinstance(u, str) and u.startswith(("http", "/"))
                            ]
                            break
                if js_links:
                    for link in js_links:
                        raw_links_set.add(link)
                    self._log(f"    [JS DOM] {len(js_links)} lien(s) collectÃ©(s)")

            # 5. Liens dans le Markdown (sites SPA / sans JSON-LD) - DERNIER fallback
            markdown_obj = crawl_result.markdown
            if markdown_obj:
                md_text = ""
                if hasattr(markdown_obj, "raw_markdown") and markdown_obj.raw_markdown:
                    md_text = markdown_obj.raw_markdown
                elif isinstance(markdown_obj, str):
                    md_text = markdown_obj
                if md_text and len(md_text) > 100:  # Markdown pas vide
                    md_links = []
                    # [texte](url)
                    for m in re.findall(r'\]\s*\(\s*([^)\s]+)\s*\)', md_text):
                        if m.startswith(("http", "/")):
                            md_links.append(m)
                            raw_links_set.add(m)
                    # URLs absolues https?://...
                    for m in re.findall(r'https?://[^\s\]\)"\'>]+', md_text):
                        clean = m.split(")")[0].split("]")[0]
                        md_links.append(clean)
                        raw_links_set.add(clean)
                    if md_links:
                        self._log(f"    [Markdown] {len(md_links)} lien(s) dÃ©tectÃ©(s)")

            # Conversion en liste pour traitement
            raw_links = list(raw_links_set)

            # Filtrage des liens : garder seulement ceux du domaine
            for href in raw_links:
                if not href:
                    continue
                full_url = urljoin(url, href)
                parsed = urlparse(full_url)
                netloc_lower = parsed.netloc.lower()
                if (
                    netloc_lower in self._domain_set
                    and self.is_valid_url(full_url)
                ):
                    clean_link = self.normalize_url(full_url)
                    if clean_link != normalized_current:
                        links.append(clean_link)
                else:
                    self.stats["links_filtered"] += 1

            unique_links = list(set(links))
            self.stats["links_discovered"] += len(unique_links)

            # âš ï¸ DEBUG: Avertir si aucun lien dÃ©couvert sur cette page
            if not unique_links:
                self._log(f"âš ï¸  AUCUN lien dÃ©couvert sur {url}")
                self._log(f"    Total raw_links trouvÃ©s: {len(raw_links)}")
                self._log(f"    HTML size: {len(html_content)} bytes")
                self._log(f"    Domaine actuel: {self.domain}, domains acceptÃ©s: {self._domain_set}")

            # â”€â”€ JSON-LD (double extraction fusionnÃ©e) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            json_ld_soup = self._extract_jsonld_from_soup(soup)
            json_ld_raw = self._extract_jsonld_from_html(html_content)
            json_ld_data = self._merge_jsonld_no_duplicates(json_ld_soup, json_ld_raw)

            # â”€â”€ Markdown (NOUVEAU â€” LLM-ready) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            markdown_obj = crawl_result.markdown
            raw_md = ""
            fit_md = ""
            if markdown_obj:
                if hasattr(markdown_obj, "raw_markdown"):
                    raw_md = markdown_obj.raw_markdown or ""
                elif isinstance(markdown_obj, str):
                    raw_md = markdown_obj
                if hasattr(markdown_obj, "fit_markdown"):
                    fit_md = markdown_obj.fit_markdown or ""

            # â”€â”€ Temps de rÃ©ponse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            response_time = getattr(crawl_result, "response_headers", {})
            # Crawl4AI ne donne pas de response_time direct, on estime 0
            response_time = 0.0

            return {
                # â”€â”€ ClÃ©s V1 (100% compatibles) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                "url": url,
                "title": final_title,
                "links": unique_links,
                "description": meta_desc,
                "h1": h1,
                "response_time": response_time,
                "html_content": html_content,
                "last_modified": "",
                "has_structured_data": bool(json_ld_data),
                "json_ld": json_ld_data,
                "h2_count": len(soup.find_all("h2")),
                "lists_count": len(soup.find_all(["ul", "ol"])),
                # â”€â”€ ClÃ©s NOUVELLES V2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                "markdown": raw_md,           # Page complÃ¨te en Markdown
                "fit_markdown": fit_md,        # Contenu core uniquement (LLM)
                "crawl4ai_success": crawl_result.success,
            }

        except Exception as e:
            self._log(f"Erreur _build_page_result({url}): {e}")
            self.stats["errors"] += 1
            return None

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  CONFIGURATION CRAWL4AI
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _get_browser_config(self) -> BrowserConfig:
        """Configure le navigateur Playwright."""
        kwargs = dict(
            headless=True,
            verbose=False,
        )
        if self.proxy:
            kwargs["proxy"] = {"server": self.proxy}
        return BrowserConfig(**kwargs)

    def _get_run_config(self, cache_mode=None) -> CrawlerRunConfig:
        """Configure une exÃ©cution de crawl."""
        if cache_mode is None:
            cache_mode = CacheMode.ENABLED if self.cache else CacheMode.BYPASS

        # JS : forcer l'extraction des liens depuis le DOM rendu (sites SPA / sans JSON-LD)
        # âœ… Collecte TOUS les liens <a> pour fallback en cas oÃ¹ Crawl4AI n'en extrait pas
        js_collect_links = (
            "(() => { "
            "var out = []; "
            "try { "
            "document.querySelectorAll('a[href]').forEach(a => { "
            "var h = a.href; "
            "if (h && !h.startsWith('javascript:') && !h.startsWith('mailto:') && !h.startsWith('tel:')) out.push(h); "
            "}); "
            "} catch(e) {} "
            "return out; "
            "})()"
        )
        return CrawlerRunConfig(
            cache_mode=cache_mode,
            page_timeout=PAGE_TIMEOUT_MS,
            # â±ï¸ DÃ©lai pour laisser le JS se charger (4s pour SPA, 2s pour sites statiques)
            delay_before_return_html=4.0,
            # ğŸ“„ Scanner la page complÃ¨te (hauteur complÃ¨te) pour dÃ©couvrir les liens au scroll
            scan_full_page=True,
            # ğŸ”„ DÃ©lai entre chaque scroll pour laisser le contenu se charger
            scroll_delay=0.3,
            # âœ… JS pour collecter les liens du DOM rendu
            js_code=js_collect_links,
            # ğŸ­ Simuler un utilisateur pour Ã©viter les anti-bots
            simulate_user=True,
            # âœ¨ Magic mode pour meilleure dÃ©tection du contenu
            magic=True,
            # ğŸ“Š Seuil minimum de mots pour considÃ©rer le contenu valide
            word_count_threshold=10,
            # ğŸ—‘ï¸ Supprimer les Ã©lÃ©ments overlay (popups, modals, etc)
            remove_overlay_elements=True,
            # âš ï¸ FIXE: exclude_external_links=False pour voir TOUS les liens
            # (On va filtrer par domaine manuellement aprÃ¨s)
            exclude_external_links=False,
        )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  CRAWL PRINCIPAL (async)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    async def _crawl_async(
        self,
        progress_callback: Callable = None,
    ) -> List[Dict]:
        """
        Crawl principal async avec Crawl4AI.
        BFS sur le domaine, concurrence configurable.
        """
        browser_config = self._get_browser_config()
        run_config = self._get_run_config()

        # File BFS manuelle pour contrÃ´le fin + compatibilitÃ© V1
        queue = list(self.start_urls)
        self.visited.update(self.start_urls)
        results = []
        crawled_count = 0

        self._log(f"\n{'='*60}")
        self._log(f"CRAWL V2 â€” max {self.max_urls} pages | {self.concurrency} en parallÃ¨le")
        self._log(f"{'='*60}\n")

        async with AsyncWebCrawler(config=browser_config) as crawler:

            while queue and crawled_count < self.max_urls:
                # Prend un batch de N URLs
                batch_size = min(self.concurrency, self.max_urls - crawled_count, len(queue))
                batch = [queue.pop(0) for _ in range(batch_size)]

                percent = min(crawled_count / self.max_urls, 0.99)
                if progress_callback:
                    progress_callback(
                        f"{crawled_count}/{self.max_urls} | Queue: {len(queue)}",
                        percent,
                    )

                self._log(f"Batch {crawled_count+1}â€“{crawled_count+batch_size} | Queue: {len(queue)}")

                # Crawl du batch en parallÃ¨le
                tasks = [
                    crawler.arun(url=url, config=run_config)
                    for url in batch
                ]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                for url, cr in zip(batch, batch_results):
                    if isinstance(cr, Exception):
                        self._log(f"  âŒ {url} â†’ {cr}")
                        self.stats["errors"] += 1
                        self.stats["pages_skipped"] += 1
                        continue

                    if not cr or not cr.success:
                        reason = getattr(cr, "error_message", "unknown") if cr else "null result"
                        self._log(f"  âš ï¸  {url} â†’ {reason}")
                        self.stats["pages_skipped"] += 1
                        continue

                    page_data = self._build_page_result(url, cr)
                    if page_data:
                        results.append(page_data)
                        crawled_count += 1
                        self.stats["pages_crawled"] += 1

                        json_ld_count = len(page_data.get("json_ld", []))
                        discovered_links = len(page_data["links"])

                        self._log(
                            f"  âœ… {page_data['title'][:40]} "
                            f"| {json_ld_count} JSON-LD "
                            f"| {discovered_links} liens dÃ©couverts"
                        )

                        # Ajoute les nouveaux liens Ã  la queue
                        new_links_added = 0
                        for link in page_data["links"]:
                            if link in self.visited:
                                self.stats["links_duplicate"] += 1
                            elif len(queue) < MAX_QUEUE_LINKS:
                                self.visited.add(link)
                                queue.append(link)
                                new_links_added += 1

                        # Log si des liens ont Ã©tÃ© ajoutÃ©s Ã  la queue
                        if new_links_added > 0:
                            self._log(f"     â†’ {new_links_added} lien(s) ajoutÃ©(s) Ã  la queue (queue size: {len(queue)})")
                        elif discovered_links > 0:
                            self._log(f"     âš ï¸  {discovered_links} lien(s) dÃ©couvert(s) mais aucun ajoutÃ© (tous en doublons)")
                    else:
                        self.stats["pages_skipped"] += 1

        self.results = results
        return results

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  ANALYSE PATTERNS (identique V1)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def analyze_patterns(self, pages: List[Dict]) -> List[Dict]:
        """Analyse les patterns d'URL (identique V1)."""
        groups = {}
        for p in pages:
            path = urlparse(p["url"]).path
            segments = [s for s in path.split("/") if s]

            if segments:
                group_key = segments[0]
                if group_key in ["fr", "en", "de", "es"] and len(segments) > 1:
                    group_key = segments[1]
            else:
                group_key = "Accueil"

            if group_key not in groups:
                groups[group_key] = {"count": 0, "samples": [], "name": group_key}

            groups[group_key]["count"] += 1
            groups[group_key]["samples"].append(p)

        return list(groups.values())

    def get_pattern_summary(self):
        return self.analyze_patterns(self.results)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  INTERFACE PUBLIQUE (identique V1)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def run_analysis(
        self,
        progress_callback: Callable = None,
        log_callback: Callable = None,
    ):
        """
        Interface synchrone â€” compatible drop-in avec SmartScraper.run_analysis().
        Retourne (results, summary) identiques Ã  V1.
        """
        if log_callback:
            self.log_callback = log_callback

        # Lance le crawl async dans une boucle event
        try:
            loop = asyncio.get_event_loop()
            if loop.is_closed():
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        results = loop.run_until_complete(
            self._crawl_async(progress_callback=progress_callback)
        )

        patterns = self.analyze_patterns(results)

        self._log(f"\nâœ… TERMINÃ‰ : {self.stats['pages_crawled']} pages crawlÃ©es")
        self._log(f"   JSON-LD trouvÃ©s : {sum(len(r.get('json_ld',[])) for r in results)}")
        self._log(f"   Avec structured data : {sum(1 for r in results if r.get('has_structured_data'))}")

        if progress_callback:
            progress_callback(f"TerminÃ©: {self.stats['pages_crawled']}", 1.0)

        return results, {
            "total_urls": len(results),
            "patterns": len(patterns),
            "stats": self.stats,
            "filtered_log": self.filtered_log,
            "duplicate_log": self.duplicate_log,
        }

    async def run_analysis_async(
        self,
        progress_callback: Callable = None,
        log_callback: Callable = None,
    ):
        """
        Interface async native â€” pour Streamlit avec asyncio natif.
        MÃªme retour que run_analysis().
        """
        if log_callback:
            self.log_callback = log_callback

        results = await self._crawl_async(progress_callback=progress_callback)
        patterns = self.analyze_patterns(results)

        if progress_callback:
            progress_callback(f"TerminÃ©: {self.stats['pages_crawled']}", 1.0)

        return results, {
            "total_urls": len(results),
            "patterns": len(patterns),
            "stats": self.stats,
            "filtered_log": self.filtered_log,
            "duplicate_log": self.duplicate_log,
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  HELPER STANDALONE (compatible fetch_page de V1)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def fetch_page_async(url: str, timeout: int = 15) -> str:
    """RÃ©cupÃ¨re le HTML d'une seule page via Crawl4AI (async)."""
    if not url.startswith(("http://", "https://")):
        url = "https://" + url
    config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        page_timeout=timeout * 1000,
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url, config=config)
        return result.html or ""


def fetch_page(url: str, timeout: int = 15) -> str:
    """RÃ©cupÃ¨re le HTML d'une seule page (interface sync â€” compatible V1)."""
    return asyncio.run(fetch_page_async(url, timeout))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ALIAS DE COMPATIBILITÃ‰ (import transparent)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Pour un swap total sans changer les imports dans le reste de HOTARU :
# from core.scraping_v2 import SmartScraper
SmartScraper = HotaruScraperV2


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  DÃ‰MO CLI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import sys

    url = sys.argv[1] if len(sys.argv) > 1 else "https://example.com"
    max_urls = int(sys.argv[2]) if len(sys.argv) > 2 else 10

    print(f"\nğŸ” Test HotaruScraperV2")
    print(f"   URL    : {url}")
    print(f"   Max    : {max_urls} pages\n")

    scraper = HotaruScraperV2(
        start_urls=url,
        max_urls=max_urls,
        concurrency=5,
        cache=False,
    )

    results, summary = scraper.run_analysis()

    print(f"\n{'='*60}")
    print(f"RÃ‰SULTATS")
    print(f"{'='*60}")
    print(f"Pages crawlÃ©es  : {summary['total_urls']}")
    print(f"Patterns        : {summary['patterns']}")
    print(f"Erreurs         : {summary['stats']['errors']}")
    print(f"Liens dÃ©couverts: {summary['stats']['links_discovered']}")

    for r in results[:3]:
        print(f"\n  ğŸ“„ {r['title']}")
        print(f"     URL      : {r['url']}")
        print(f"     H1       : {r['h1'][:60]}")
        print(f"     JSON-LD  : {len(r['json_ld'])} bloc(s)")
        print(f"     Markdown : {len(r.get('fit_markdown',''))} chars")
        if r["json_ld"]:
            for block in r["json_ld"][:2]:
                t = block.get("@type", "?") if isinstance(block, dict) else "Array"
                print(f"       â€¢ @type: {t}")