# =============================================================================
# AUDIT GEO - VERSION AVEC LOGS EN TEMPS R√âEL
# =============================================================================


import streamlit as st
import json
import re
import requests
import zlib
import base64
from urllib.parse import urlparse
from collections import defaultdict
import networkx as nx
from pyvis.network import Network
import streamlit.components.v1 as components
from bs4 import BeautifulSoup
from core.database import AuditDatabase
from core.scraping import SmartScraper

# =============================================================================
# VERSION 2.7.0 - LOGS + COMPTEURS + STATS D√âTAILL√âES
# =============================================================================

# =============================================================================
# 1. STYLE & CONFIGURATION
# =============================================================================
def inject_hotaru_css():
    """CSS is now centralized in assets/style.css - this is a no-op kept for interface stability."""
    pass

# =============================================================================
# 2. FONCTIONS TECHNIQUES (SCORING & INFRA)
# =============================================================================

def check_geo_infrastructure(base_url):
    domain = base_url.rstrip('/')
    assets = {
        "robots.txt": {"url": f"{domain}/robots.txt", "desc": "Autorise GPTBot et les crawlers IA."},
        "sitemap.xml": {"url": f"{domain}/sitemap.xml", "desc": "Guide d'indexation pour les moteurs de r√©ponse."},
        "llms.txt": {"url": f"{domain}/llms.txt", "desc": "Standard 2025 pour la consommation LLM."},
        "JSON-LD": {"url": domain, "desc": "Donn√©es structur√©es (Entit√©s de marque)."}
    }
    results = {}; score = 0
    for name, data in list(assets.items())[:3]:
        try:
            r = requests.get(data['url'], timeout=3)
            found = (r.status_code == 200)
            results[name] = {"status": found, "meta": data}
            if found: score += 25
        except: results[name] = {"status": False, "meta": data}
    try:
        r = requests.get(domain, timeout=5); soup = BeautifulSoup(r.text, 'html.parser')
        has_json = bool(soup.find('script', type='application/ld+json'))
        results["JSON-LD"] = {"status": has_json, "meta": assets["JSON-LD"]}
        if has_json: score += 25
    except: results["JSON-LD"] = {"status": False, "meta": assets["JSON-LD"]}
    return results, score

def fetch_file_content(base_url, filename):
    """R√©cup√®re le contenu d'un fichier (robots.txt, llms.txt) sur le site"""
    url = f"{base_url.rstrip('/')}/{filename}"
    try:
        r = requests.get(url, timeout=5)
        if r.status_code == 200:
            return r.text.strip(), True
        else:
            return f"# {filename} non trouv√© (HTTP {r.status_code})", False
    except Exception as e:
        return f"# Erreur de r√©cup√©ration : {e}", False

def generate_mistral_optimization(file_content, filename, site_url, found):
    """Appelle Mistral pour g√©n√©rer une version optimis√©e du fichier"""
    try:
        api_key = st.secrets["mistral"]["api_key"]
    except Exception:
        return "Cl√© API Mistral manquante dans les secrets Streamlit."

    if filename == "robots.txt":
        prompt = f"""Tu es un expert en SEO technique et AI-Readability.
Voici le fichier robots.txt actuel du site {site_url} :

{file_content if found else "Le fichier robots.txt n'existe pas ou est vide."}

G√©n√®re un robots.txt PARFAIT et optimis√© pour les agents IA (GPTBot, ChatGPT-User, Google-Extended, ClaudeBot, PerplexityBot, Bingbot).
Le fichier doit :
1. Autoriser explicitement les crawlers IA importants
2. R√©f√©rencer le sitemap.xml
3. Bloquer les ressources inutiles (/admin, /wp-login, etc.)
4. Suivre les meilleures pratiques 2025-2026

R√©ponds UNIQUEMENT avec le contenu du fichier robots.txt, sans aucun commentaire ni explication autour."""
    else:  # llms.txt
        prompt = f"""Tu es un expert en AI-Readability et standards LLM.
Voici le fichier llms.txt actuel du site {site_url} :

{file_content if found else "Le fichier llms.txt n'existe pas."}

G√©n√®re un llms.txt PARFAIT selon le standard llms.txt 2025.
Le fichier doit :
1. Pr√©senter clairement l'organisation/entreprise
2. Lister les pages cl√©s et sections du site
3. Fournir le contexte m√©tier pour les LLMs
4. Inclure les informations de contact et liens importants
5. Suivre la sp√©cification llms.txt (https://llmstxt.org/)

Le site est : {site_url}

R√©ponds UNIQUEMENT avec le contenu du fichier llms.txt, sans aucun commentaire ni explication autour."""

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "Accept": "application/json"
    }

    payload = {
        "model": "mistral-small-latest",
        "messages": [
            {"role": "system", "content": "Tu es un expert en optimisation de sites web pour les agents IA. Tu g√©n√®res uniquement du code/config, jamais d'explications."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.2,
        "max_tokens": 2000
    }

    try:
        response = requests.post("https://api.mistral.ai/v1/chat/completions", headers=headers, json=payload)
        response.raise_for_status()
        result = response.json()
        return result["choices"][0]["message"]["content"].strip()
    except Exception as e:
        return f"Erreur API Mistral : {e}"

def render_mistral_optimization(base_url):
    """Affiche le module d'optimisation IA avec comparatif existant vs proposition Mistral"""
    if not base_url:
        return

    with st.expander("ü§ñ Optimisation IA (Mistral) ‚Äî robots.txt & llms.txt", expanded=False):
        st.caption("Analyse et optimisation automatique de vos fichiers d'infrastructure IA")

        files_to_optimize = ["robots.txt", "llms.txt"]
        tabs = st.tabs([f"üìÑ {f}" for f in files_to_optimize])

        for idx, filename in enumerate(files_to_optimize):
            with tabs[idx]:
                # R√©cup√©rer le contenu existant
                content, found = fetch_file_content(base_url, filename)

                if found:
                    st.success(f"{filename} trouv√© sur le site")
                else:
                    st.error(f"{filename} absent ou inaccessible (404)")

                # Bouton pour lancer l'optimisation
                cache_key = f"mistral_opt_{filename}"
                if st.button(f"Optimiser {filename} avec Mistral", key=f"btn_{filename}", use_container_width=True):
                    with st.spinner(f"Mistral analyse {filename}..."):
                        optimized = generate_mistral_optimization(content, filename, base_url, found)
                        st.session_state[cache_key] = optimized

                # Affichage c√¥te √† c√¥te
                col_left, col_right = st.columns(2)

                with col_left:
                    st.markdown(f"**Existant** {'‚úÖ' if found else '‚ùå'}")
                    st.code(content if found else f"# {filename} inexistant", language="text")

                with col_right:
                    st.markdown("**Proposition Mistral** ü§ñ")
                    if cache_key in st.session_state:
                        st.code(st.session_state[cache_key], language="text")
                    else:
                        st.info("Cliquez sur le bouton ci-dessus pour g√©n√©rer une proposition optimis√©e.")

def calculate_page_score(page):
    """
    Calcule le score GEO avanc√© d'une page
    Utilise le nouveau syst√®me de scoring multicrit√®re
    """
    try:
        from modules.geo_scoring import GEOScorer
        scorer = GEOScorer()
        result = scorer.calculate_score(page)
        return result['total_score'], result['grade'], result['breakdown'], result['recommendations']
    except:
        # Fallback si geo_scoring n'existe pas
        score = 70  # Score par d√©faut
        grade = 'B'
        breakdown = {}
        recommendations = []
        return score, grade, breakdown, recommendations

def get_clean_label(title, url, domain):
    try:
        clean = re.split(r' [-|:|‚Ä¢] ', title)[0]
        if len(clean) < 4: clean = url.rstrip('/').split('/')[-1].replace('-', ' ').capitalize()
        return clean[:20] + ".." if len(clean) > 22 else clean
    except: return "Page"

# =============================================================================
# 2b. CAT√âGORISATION INTELLIGENTE DES URLs
# =============================================================================

# Dictionnaire de patterns -> cat√©gories pour la classification des URLs
URL_CATEGORY_PATTERNS = {
    "Offres d'emploi": ['metier', 'emploi', 'offre', 'recrutement', 'carriere', 'poste', 'job', 'candidat', 'postuler'],
    "T√©moignages": ['temoignage', 'portrait', 'parcours', 'interview', 'histoire', 'vecu', 'retour-experience'],
    "Actualit√©s": ['actualite', 'actu', 'news', 'blog', 'article', 'presse', 'communique', 'evenement'],
    "Formation": ['formation', 'ecole', 'cursus', 'diplome', 'stage', 'apprentissage', 'etude'],
    "Pr√©sentation": ['decouvrir', 'qui-sommes', 'a-propos', 'about', 'presentation', 'mission', 'valeur', 'histoire'],
    "FAQ / Aide": ['faq', 'aide', 'question', 'contact', 'support', 'assistance'],
    "L√©gal": ['mention', 'legal', 'cgu', 'cgv', 'confidentialite', 'cookie', 'politique', 'condition', 'rgpd'],
    "M√©dias": ['media', 'video', 'photo', 'galerie', 'image', 'reportage', 'documentaire'],
    "Espace candidat": ['espace', 'compte', 'profil', 'inscription', 'connexion', 'login', 'dashboard'],
}

def categorize_urls(results):
    """Classe les URLs crawl√©es par cat√©gories intelligentes"""
    categories = defaultdict(list)

    for page in results:
        url = page.get('url', '')
        title = (page.get('title', '') or '').lower()
        h1 = (page.get('h1', '') or '').lower()
        path = urlparse(url).path.lower()

        # Texte combin√© pour la recherche
        search_text = f"{path} {title} {h1}"

        matched = False
        for category, keywords in URL_CATEGORY_PATTERNS.items():
            if any(kw in search_text for kw in keywords):
                categories[category].append(page)
                matched = True
                break

        if not matched:
            # Fallback : cat√©goriser par premier segment de chemin
            segments = [s for s in path.split('/') if s and s not in ['fr', 'en', 'de', 'es', 'www']]
            if segments:
                group = segments[0].replace('-', ' ').replace('_', ' ').title()
                categories[f"üìÇ {group}"].append(page)
            else:
                categories["Pages statiques"].append(page)

    return dict(sorted(categories.items(), key=lambda x: -len(x[1])))

def render_url_journal(results):
    """Affiche le journal des URLs crawl√©es class√©es par cat√©gories"""
    categories = categorize_urls(results)

    st.markdown("#### üìã Journal des pages crawl√©es")
    st.caption(f"{len(results)} pages class√©es en {len(categories)} cat√©gories")

    for cat_name, pages in categories.items():
        with st.expander(f"{cat_name} ({len(pages)} pages)", expanded=False):
            for p in pages:
                score_data = calculate_page_score(p)
                if isinstance(score_data, tuple):
                    sc, grade, _, _ = score_data
                else:
                    sc, grade = score_data, 'N/A'

                # Couleur selon score (s√©v√®re)
                if sc >= 95:
                    badge_color = "#10b981"
                elif sc >= 50:
                    badge_color = "#f97316"
                else:
                    badge_color = "#ef4444"

                title = p.get('title', 'Sans titre')
                url = p.get('url', '')
                st.markdown(
                    f'<div style="display:flex;align-items:center;gap:10px;padding:6px 0;border-bottom:1px solid #f1f5f9;">'
                    f'<span style="background:{badge_color};color:white;padding:2px 8px;border-radius:4px;font-size:0.75rem;font-weight:700;min-width:32px;text-align:center;">{grade}</span>'
                    f'<span style="font-size:0.9rem;font-weight:500;">{title}</span>'
                    f'<span style="font-size:0.75rem;color:#94a3b8;margin-left:auto;max-width:300px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;">{url}</span>'
                    f'</div>',
                    unsafe_allow_html=True
                )

# =============================================================================
# 3. RENDU DU GRAPHE (FIX VRAI PLEIN √âCRAN + L√âGENDE)
# =============================================================================

def render_interactive_graph(G, show_health=False):
    nt = Network(height="850px", width="100%", bgcolor="#ffffff", font_color="#0f172a")
    nt.from_nx(G)
    
    opts = {
        "nodes": {
            "font": {
                "face": "Inter, sans-serif",
                "size": 14,
                "strokeWidth": 3,
                "strokeColor": "#ffffff",
                "color": "#0f172a"
            },
            "borderWidth": 2,
            "borderWidthSelected": 3
        },
        "edges": {
            "color": "#cbd5e1",
            "smooth": {
                "type": "dynamic",
                "roundness": 0.2
            },
            "width": 1.5
        },
        "interaction": {
            "hover": True,
            "navigationButtons": True,
            "keyboard": True,
            "zoomView": True,
            "dragView": True,
            "dragNodes": True
        },
        "physics": {
            "forceAtlas2Based": {
                "gravitationalConstant": -100,
                "centralGravity": 0.01,
                "springLength": 200,
                "springConstant": 0.08,
                "avoidOverlap": 1
            },
            "solver": "forceAtlas2Based",
            "stabilization": {
                "enabled": True,
                "iterations": 200
            }
        }
    }

    nt.set_options(json.dumps(opts))
    path = "temp_graph.html"
    nt.save_graph(path)
    
    with open(path, "r", encoding="utf-8") as f:
        html = f.read()
    
    # L√âGENDE MODE SANT√â
    legend_html = ""
    if show_health:
        legend_html = """
        <div id="health-legend" style="
            position: absolute;
            bottom: 20px;
            left: 20px;
            background: white;
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.15);
            font-family: 'Inter', sans-serif;
            z-index: 9998;
        ">
            <div style="font-weight: 600; margin-bottom: 10px; font-size: 13px;">Score AI-READABLE</div>
            <div style="display: flex; align-items: center; margin-bottom: 6px;">
                <div style="width: 16px; height: 16px; background: #10b981; border-radius: 50%; margin-right: 8px;"></div>
                <span style="font-size: 12px;">95-100 : Optimis√© IA</span>
            </div>
            <div style="display: flex; align-items: center; margin-bottom: 6px;">
                <div style="width: 16px; height: 16px; background: #f97316; border-radius: 50%; margin-right: 8px;"></div>
                <span style="font-size: 12px;">50-94 : Non optimis√©</span>
            </div>
            <div style="display: flex; align-items: center;">
                <div style="width: 16px; height: 16px; background: #ef4444; border-radius: 50%; margin-right: 8px;"></div>
                <span style="font-size: 12px;">0-49 : Critique</span>
            </div>
        </div>
        """
    
    # CODE JS AM√âLIOR√â
    custom_code = f"""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap');
        
        * {{
            font-family: 'Inter', sans-serif !important;
        }}
        
        body, html {{
            font-family: 'Inter', sans-serif !important;
            color: #0f172a;
            margin: 0;
            padding: 0;
        }}
        
        .vis-network canvas {{
            font-family: 'Inter', sans-serif !important;
        }}
        
        /* VRAI PLEIN √âCRAN */
        #mynetwork:fullscreen {{
            width: 100vw !important;
            height: 100vh !important;
        }}
        
        #mynetwork:-webkit-full-screen {{
            width: 100vw !important;
            height: 100vh !important;
        }}
        
        #mynetwork:-moz-full-screen {{
            width: 100vw !important;
            height: 100vh !important;
        }}
        
        #mynetwork:-ms-fullscreen {{
            width: 100vw !important;
            height: 100vh !important;
        }}
    </style>
    <script>
        // Gestion des clics sur les n≈ìuds
        network.on("click", function (params) {{
            if (params.nodes.length > 0) {{
                var nodeId = params.nodes[0];
                if (nodeId.startsWith('http')) {{
                    window.open(nodeId, '_blank');
                }}
            }}
        }});
        
        // VRAI PLEIN √âCRAN sur #mynetwork directement
        function toggleFullscreen() {{
            var elem = document.getElementById('mynetwork');
            if (!document.fullscreenElement && !document.mozFullScreenElement && 
                !document.webkitFullscreenElement && !document.msFullscreenElement) {{
                if (elem.requestFullscreen) {{
                    elem.requestFullscreen();
                }} else if (elem.msRequestFullscreen) {{
                    elem.msRequestFullscreen();
                }} else if (elem.mozRequestFullScreen) {{
                    elem.mozRequestFullScreen();
                }} else if (elem.webkitRequestFullscreen) {{
                    elem.webkitRequestFullscreen();
                }}
            }} else {{
                if (document.exitFullscreen) {{
                    document.exitFullscreen();
                }} else if (document.msExitFullscreen) {{
                    document.msExitFullscreen();
                }} else if (document.mozCancelFullScreen) {{
                    document.mozCancelFullScreen();
                }} else if (document.webkitExitFullscreen) {{
                    document.webkitExitFullscreen();
                }}
            }}
        }}
        
        // Ajout du bouton plein √©cran
        var fullscreenBtn = document.createElement('button');
        fullscreenBtn.innerHTML = '‚õ∂ Plein √©cran';
        fullscreenBtn.style.cssText = `
            position: fixed;
            top: 20px;
            right: 20px;
            z-index: 9999;
            padding: 10px 20px;
            background: #0f172a;
            color: white;
            border: none;
            border-radius: 8px;
            font-family: 'Inter', sans-serif;
            font-size: 14px;
            font-weight: 500;
            cursor: pointer;
            box-shadow: 0 1px 2px rgba(0,0,0,0.1);
            transition: all 0.2s ease;
        `;
        fullscreenBtn.onmouseover = function() {{
            this.style.background = '#1e293b';
            this.style.transform = 'translateY(-1px)';
            this.style.boxShadow = '0 4px 6px rgba(0,0,0,0.15)';
        }};
        fullscreenBtn.onmouseout = function() {{
            this.style.background = '#0f172a';
            this.style.transform = 'translateY(0)';
            this.style.boxShadow = '0 1px 2px rgba(0,0,0,0.1)';
        }};
        fullscreenBtn.onclick = toggleFullscreen;
        document.body.appendChild(fullscreenBtn);
    </script>
    {legend_html}
    """
    
    components.html(html.replace("</body>", custom_code + "</body>"), height=900)

# =============================================================================
# 4. ONGLET M√âTHODOLOGIE
# =============================================================================

def render_methodologie():
    """M√©thodologie Hotaru - Version modulaire int√©gr√©e √† l'onglet AUDIT"""

    st.markdown("""
    <style>
        .methodo-container { max-width: 900px; margin: auto; padding: 20px; }
        .methodo-title { font-size: 2.8rem; font-weight: 800; letter-spacing: -0.04em; margin-bottom: 0.2rem; color: #000; }
        .methodo-subtitle { font-size: 1.1rem; color: #94a3b8; margin-bottom: 4rem; font-weight: 400; text-transform: uppercase; letter-spacing: 0.1em; }
        .methodo-header { font-size: 0.8rem; font-weight: 700; text-transform: uppercase; letter-spacing: 0.2em; color: #000; margin-bottom: 2rem; border-bottom: 2px solid #000; padding-bottom: 8px; width: fit-content; }
        .methodo-card { background: #ffffff; border: 1px solid #e2e8f0; padding: 30px; margin-bottom: -1px; transition: all 0.2s ease; }
        .methodo-card:hover { background: #f8fafc; z-index: 10; position: relative; }
        .methodo-badge { font-size: 0.65rem; font-weight: 800; color: #64748b; border: 1px solid #e2e8f0; padding: 2px 8px; margin-bottom: 15px; display: inline-block; }
        .methodo-grade-row { display: flex; justify-content: space-between; padding: 15px 0; border-bottom: 1px solid #f1f5f9; }
        .methodo-grade-letter { font-weight: 800; font-size: 1.2rem; }
        .methodo-grade-range { font-family: monospace; color: #64748b; }
        .methodo-health { border: 1px solid #000; padding: 40px; margin: 50px 0; background: #fff; }
        .methodo-dot { height: 10px; width: 10px; border-radius: 50%; display: inline-block; margin-right: 10px; }
        .methodo-tips { list-style: none; padding: 0; }
        .methodo-tips li { padding: 15px 0; border-bottom: 1px solid #f1f5f9; color: #000; font-size: 1rem; display: flex; align-items: center; }
        .methodo-tips li::before { content: ""; width: 12px; height: 1px; background: #000; margin-right: 20px; }
    </style>
    """, unsafe_allow_html=True)

    st.markdown('<div class="methodo-container">', unsafe_allow_html=True)
    st.markdown('<div class="methodo-title">M√âTHODOLOGIE HOTARU</div>', unsafe_allow_html=True)
    st.markdown('<div class="methodo-subtitle">2026 Framework</div>', unsafe_allow_html=True)

    # 01. CONCEPT
    st.markdown('<div class="methodo-header">01. CONCEPT</div>', unsafe_allow_html=True)
    st.write(
        "L'optimisation des actifs s√©mantiques pour la citation directe par les LLMs. "
        "Le score Hotaru mesure la capacit√© d'un contenu √† √™tre extrait et valid√© par les moteurs g√©n√©ratifs."
    )
    st.markdown('<div style="margin-bottom:4rem;"></div>', unsafe_allow_html=True)

    # 02. CRIT√àRES
    st.markdown('<div class="methodo-header">02. CRIT√àRES D\'ANALYSE</div>', unsafe_allow_html=True)

    col1, col2 = st.columns(2)
    with col1:
        st.markdown("""
        <div class="methodo-card">
            <div class="methodo-badge">15 PTS</div>
            <div style="font-weight:700; font-size:1.1rem; margin-bottom:8px;">Meta Description</div>
            <div style="font-size:0.9rem; color:#64748b; line-height:1.5;">Pr√©cision s√©mantique du r√©sum√© pour le crawling par les agents IA.</div>
        </div>
        <div class="methodo-card">
            <div class="methodo-badge">15 PTS</div>
            <div style="font-weight:700; font-size:1.1rem; margin-bottom:8px;">Donn√©es Structur√©es</div>
            <div style="font-size:0.9rem; color:#64748b; line-height:1.5;">Sch√©mas JSON-LD, identification des entit√©s et relations.</div>
        </div>
        <div class="methodo-card">
            <div class="methodo-badge">20 PTS</div>
            <div style="font-weight:700; font-size:1.1rem; margin-bottom:8px;">Architecture S√©mantique</div>
            <div style="font-size:0.9rem; color:#64748b; line-height:1.5;">Logique de titrage Hn et structuration par listes/tableaux.</div>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown("""
        <div class="methodo-card">
            <div class="methodo-badge">15 PTS</div>
            <div style="font-weight:700; font-size:1.1rem; margin-bottom:8px;">Profondeur & Sources</div>
            <div style="font-size:0.9rem; color:#64748b; line-height:1.5;">Richesse textuelle et autorit√© des maillages externes.</div>
        </div>
        <div class="methodo-card">
            <div class="methodo-badge">10 PTS</div>
            <div style="font-weight:700; font-size:1.1rem; margin-bottom:8px;">Richesse en Entit√©s</div>
            <div style="font-size:0.9rem; color:#64748b; line-height:1.5;">Extraction de faits, dates et donn√©es propri√©taires.</div>
        </div>
        <div class="methodo-card">
            <div class="methodo-badge">25 PTS</div>
            <div style="font-weight:700; font-size:1.1rem; margin-bottom:8px;">Technique IA-Ready</div>
            <div style="font-size:0.9rem; color:#64748b; line-height:1.5;">Accessibilit√© bots et pr√©sence du standard llms.txt.</div>
        </div>
        """, unsafe_allow_html=True)

    # 03. SCORING
    st.markdown('<div style="margin-bottom:4rem;"></div>', unsafe_allow_html=True)
    st.markdown('<div class="methodo-header">03. SCORING SYSTEM</div>', unsafe_allow_html=True)

    st.markdown("""
    <div class="methodo-grade-row"><span class="methodo-grade-letter">A+</span><span class="methodo-grade-range">90 - 100</span></div>
    <div class="methodo-grade-row"><span class="methodo-grade-letter">A</span><span class="methodo-grade-range">80 - 89</span></div>
    <div class="methodo-grade-row"><span class="methodo-grade-letter">B</span><span class="methodo-grade-range">70 - 79</span></div>
    <div class="methodo-grade-row"><span class="methodo-grade-letter">C</span><span class="methodo-grade-range">50 - 69</span></div>
    <div class="methodo-grade-row"><span class="methodo-grade-letter">F</span><span class="methodo-grade-range"> &lt; 50</span></div>
    """, unsafe_allow_html=True)

    # Health monitoring
    st.markdown("""
    <div class="methodo-health">
        <div style="font-weight:800; text-transform:uppercase; font-size:0.7rem; letter-spacing:0.2em; margin-bottom:2rem;">Health Monitoring</div>
        <div style="display:flex; gap:30px;">
            <div><span class="methodo-dot" style="background:#22c55e;"></span><span style="font-size:0.9rem; font-weight:600;">OPTIMAL</span></div>
            <div><span class="methodo-dot" style="background:#eab308;"></span><span style="font-size:0.9rem; font-weight:600;">AVERAGE</span></div>
            <div><span class="methodo-dot" style="background:#ef4444;"></span><span style="font-size:0.9rem; font-weight:600;">CRITICAL</span></div>
        </div>
    </div>
    """, unsafe_allow_html=True)

    # 04. DIRECTIVES
    st.markdown('<div class="methodo-header">04. STRATEGIC DIRECTIVES</div>', unsafe_allow_html=True)
    st.markdown("""
    <ul class="methodo-tips">
        <li>Privil√©gier les formats factuels (tableaux, data-points).</li>
        <li>Convertir les paragraphes denses en listes structur√©es.</li>
        <li>Impl√©menter le balisage JSON-LD sp√©cifique.</li>
        <li>Utiliser des titres sous forme de questions directes.</li>
        <li>Maintenir une fra√Æcheur de donn√©e &lt; 90 jours.</li>
    </ul>
    """, unsafe_allow_html=True)

    st.markdown('</div>', unsafe_allow_html=True)

# =============================================================================
# 5. INTERFACE PRINCIPALE (AVEC SOUS-ONGLETS)
# =============================================================================

def render_audit_geo():
    inject_hotaru_css()
    db = AuditDatabase()
    user_email = st.session_state.user_email
    
    # SOUS-ONGLETS DANS AUDIT
    tab1, tab2 = st.tabs(["üîç Audit Site", "üìñ M√©thodologie"])
    
    # ============= TAB 1 : AUDIT SITE =============
    with tab1:
        # --- CHARGEMENT SIDEBAR ---
        all_audits = db.load_user_audits(user_email)
        
        # Extraction propre des noms de Workspaces
        ws_list = []
        for a in all_audits:
            ws_name = str(a.get('workspace', '')).strip()
            if ws_name and ws_name not in ws_list:
                ws_list.append(ws_name)
        
        if not ws_list: ws_list = ["Nouveau"]
        else: ws_list = sorted(ws_list) + ["+ Cr√©er Nouveau"]

        # Menu Workspace
        selected_ws = st.sidebar.selectbox("üìÇ Projets (Workspaces)", ws_list)
        
        # Filtrage historique
        filtered_audits = [a for a in all_audits if str(a.get('workspace', '')).strip() == selected_ws]

        # --- ZONE DE SCAN ---
        with st.expander("üöÄ LANCER UNE NOUVELLE ANALYSE", expanded="results" not in st.session_state):
            c1, c2 = st.columns([3, 1])
            
            # ‚úÖ NOUVEAU : Textarea pour URLs multiples
            url_input = c1.text_area(
                "URLs √† analyser (une par ligne)",
                placeholder="https://example.com/\nhttps://example.com/section1\nhttps://example.com/section2",
                height=100,
                help="Entrez une ou plusieurs URLs du m√™me domaine, une par ligne. Le scraper explorera toutes ces sections."
            )
            
            default_ws = "" if selected_ws == "+ Cr√©er Nouveau" else selected_ws
            ws_in = c2.text_input("Nom du Projet", value=default_ws)
            
            # SLIDER 10 √Ä 10 000 PAGES avec paliers intelligents
            limit_in = st.select_slider(
                "Nombre de pages √† analyser",
                options=[10, 25, 50, 100, 250, 500, 1000, 2500, 5000, 10000],
                value=100,
                help="Analyse de 10 √† 10 000 pages. Plus le nombre est √©lev√©, plus l'analyse sera longue."
            )
            
            if st.button("Lancer Hotaru", use_container_width=True):
                if url_input:
                    # ‚úÖ Parser les URLs (une par ligne, ignorer les lignes vides)
                    urls = [line.strip() for line in url_input.strip().split('\n') if line.strip()]
                    
                    if not urls:
                        st.error("Veuillez entrer au moins une URL")
                        return
                    
                    # ‚úÖ V√©rifier que toutes les URLs sont du m√™me domaine
                    domains = [urlparse(url).netloc for url in urls]
                    if len(set(domains)) > 1:
                        st.error(f"‚ùå Toutes les URLs doivent √™tre du m√™me domaine. Trouv√©: {', '.join(set(domains))}")
                        return
                    
                    # Prendre la premi√®re URL comme URL de base pour l'infra check
                    base_url = urls[0]
                    
                    bar = st.progress(0, "Analyse infrastructure...")
                    infra, score = check_geo_infrastructure(base_url)
                    st.session_state.geo_infra = infra
                    st.session_state.geo_score = score
                    
                    # ‚úÖ NOUVEAU : Lancement avec liste d'URLs
                    scr = SmartScraper(urls, max_urls=limit_in)
                    res, stats = scr.run_analysis(
                        progress_callback=lambda m, v: bar.progress(v, m)
                    )
                    
                    st.session_state.update({
                        "results": res, 
                        "clusters": scr.get_pattern_summary(), 
                        "target_url": base_url,  # Pour compatibilit√© avec le reste du code
                        "start_urls": urls,  # ‚úÖ NOUVEAU : Garder la liste compl√®te
                        "current_ws": ws_in if ws_in else "Non class√©",
                        "crawl_stats": stats.get('stats', {})
                    })
                    st.rerun()

        # --- SECTION ARCHIVES (DANS AUDIT) ---
        if filtered_audits:
            st.divider()
            st.subheader("üìã Archives")
            
            audit_labels = {f"{a.get('nom_site') or 'Audit'} ({a.get('date')})": a for a in filtered_audits}
            
            col1, col2 = st.columns([3, 1])
            choice = col1.selectbox("Charger un audit", list(audit_labels.keys()), label_visibility="collapsed")
            
            if col2.button("Visualiser", use_container_width=True):
                r = audit_labels[choice]
                raw_data = zlib.decompress(base64.b64decode(r['data_compressed'])).decode('utf-8')
                data = json.loads(raw_data)
                st.session_state.update({
                    "results": data['results'], 
                    "clusters": data['clusters'], 
                    "target_url": r['site_url'], 
                    "geo_infra": data.get('geo_infra', {}),
                    "geo_score": data.get('geo_score', 0),
                    "current_ws": selected_ws,
                    "crawl_stats": data.get('stats', {})
                })
                st.rerun()

        # --- AFFICHAGE DES R√âSULTATS ---
        if "results" in st.session_state:
            st.divider()
            
            # 1. Dashboard Infra avec couleur s√©v√®re
            g_score = st.session_state.get("geo_score", 0)
            if g_score >= 95:
                score_color = "#10b981"
                score_label = "Optimis√©"
            elif g_score >= 50:
                score_color = "#f97316"
                score_label = "Non optimis√©"
            else:
                score_color = "#ef4444"
                score_label = "Critique"

            st.markdown(
                f'<div style="display:flex;align-items:center;gap:12px;margin-bottom:12px;">'
                f'<h3 style="margin:0;">Score Infrastructure IA : <span style="color:{score_color};">{g_score}/100</span></h3>'
                f'<span style="background:{score_color};color:white;padding:4px 12px;border-radius:4px;font-size:0.8rem;font-weight:700;">{score_label}</span>'
                f'</div>',
                unsafe_allow_html=True
            )

            if st.session_state.get("geo_infra"):
                cols = st.columns(4)
                for i, (name, d) in enumerate(st.session_state.geo_infra.items()):
                    with cols[i]:
                        status = "status-ok" if d['status'] else "status-err"
                        txt = "OK" if d['status'] else "MISSING"
                        st.markdown(f"""<div class="infra-box"><b>{name}</b><br><span class="{status}">{txt}</span><div class="infra-desc">{d['meta']['desc']}</div></div>""", unsafe_allow_html=True)

            # ‚úÖ Module Optimisation IA (Mistral) pour robots.txt et llms.txt
            render_mistral_optimization(st.session_state.get("target_url", ""))

            st.divider()

            # ‚úÖ NOUVEAU : Stats d√©taill√©es du crawl
            if "crawl_stats" in st.session_state and st.session_state.crawl_stats:
                with st.expander("üìä Statistiques d√©taill√©es du crawl", expanded=True):
                    stats = st.session_state.crawl_stats
                    
                    # ‚úÖ Afficher les URLs de d√©part si mode multi-URLs
                    if "start_urls" in st.session_state and len(st.session_state.start_urls) > 1:
                        st.markdown(f"**üîó Points d'entr√©e : {len(st.session_state.start_urls)}**")
                        with st.expander("Voir les URLs de d√©part"):
                            for i, url in enumerate(st.session_state.start_urls, 1):
                                st.text(f"{i}. {url}")
                        st.markdown("---")
                    
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric("‚úÖ Pages crawl√©es", stats.get('pages_crawled', 0))
                        st.metric("‚ö†Ô∏è Pages ignor√©es", stats.get('pages_skipped', 0))
                    
                    with col2:
                        st.metric("üîó Liens d√©couverts", stats.get('links_discovered', 0))
                        st.metric("üö´ Liens filtr√©s", stats.get('links_filtered', 0))
                    
                    with col3:
                        st.metric("üîÑ Doublons", stats.get('links_duplicate', 0))
                        st.metric("‚õî Queue pleine", stats.get('queue_full_blocks', 0))
                    
                    with col4:
                        st.metric("‚ùå Erreurs", stats.get('errors', 0))
                        st.metric("üìç URLs visit√©es", len(st.session_state.results))
                
                st.divider()

            # ‚úÖ Journal des pages crawl√©es par cat√©gories
            with st.expander("üìã Journal des pages crawl√©es (par cat√©gories)", expanded=False):
                render_url_journal(st.session_state.results)

            st.divider()

            # 2. Commandes Graphe
            c_expert, c_save_name, c_save_btn = st.columns([1, 2, 1])
            expert_on = c_expert.toggle("Score AI-READABLE", value=False)
            
            domain = urlparse(st.session_state.target_url).netloc
            s_name = c_save_name.text_input("Nom sauvegarde", value=domain.split('.')[0], label_visibility="collapsed")
            
            # 3. Sauvegarde S√©curis√©e (Trimming + Limite)
            if c_save_btn.button("Sauvegarder", use_container_width=True):
                # ‚úÖ LIMITE : Maximum 100 pages sauvegard√©es pour √©viter erreur GSheet
                max_pages_to_save = 100
                results_to_save = st.session_state.results[:max_pages_to_save]
                
                clean_results = []
                for r in results_to_save:
                    clean_results.append({
                        "url": r.get("url"),
                        "title": r.get("title", "")[:50],  # ‚úÖ R√©duit de 100 √† 50
                        "description": r.get("description", "")[:100],  # ‚úÖ R√©duit de 200 √† 100
                        "h1": r.get("h1", "")[:50],  # ‚úÖ R√©duit de 100 √† 50
                        "response_time": round(r.get("response_time", 0), 2),  # ‚úÖ Arrondi
                        "has_structured_data": r.get("has_structured_data", False),
                        "h2_count": r.get("h2_count", 0)
                        # ‚úÖ Suppression de lists_count et html_content
                    })
                
                # ‚úÖ Limiter aussi les clusters (garder que les 5 premiers samples par cluster)
                compact_clusters = []
                for cluster in st.session_state.clusters:
                    compact_clusters.append({
                        "name": cluster["name"],
                        "count": cluster["count"],
                        "samples": []  # ‚úÖ On ne garde PAS les samples (d√©j√† dans results)
                    })
                
                payload = {
                    "results": clean_results,
                    "clusters": compact_clusters,
                    "geo_infra": st.session_state.get('geo_infra', {}),
                    "geo_score": st.session_state.get('geo_score', 0),
                    "stats": {
                        "pages_crawled": st.session_state.crawl_stats.get('pages_crawled', 0),
                        "links_discovered": st.session_state.crawl_stats.get('links_discovered', 0),
                        "start_urls_count": st.session_state.crawl_stats.get('start_urls_count', 1)
                    },
                    "start_urls": st.session_state.get('start_urls', [st.session_state.target_url])[:5]  # ‚úÖ Max 5 URLs
                }
                
                # ‚úÖ Afficher warning si limitation
                if len(st.session_state.results) > max_pages_to_save:
                    st.warning(f"‚ö†Ô∏è Seules les {max_pages_to_save} premi√®res pages sur {len(st.session_state.results)} seront sauvegard√©es (limite Google Sheets)")
                
                db.save_audit(user_email, st.session_state.current_ws, st.session_state.target_url, s_name, payload)
                st.toast("‚úÖ Audit sauvegard√© avec succ√®s")

            # 4. Construction du Graphe NetworkX
            G = nx.DiGraph()
            G.add_node(
                st.session_state.target_url, 
                label=domain.upper(), 
                size=35, 
                color="#0f172a",
                font={'color': '#ffffff', 'face': 'Inter'}
            )
            
            for c in st.session_state.clusters:
                c_id = f"group_{c['name']}"
                G.add_node(
                    c_id, 
                    label=c['name'].upper(), 
                    color="#cbd5e1",
                    size=25,
                    font={'color': '#0f172a', 'face': 'Inter'}
                )
                G.add_edge(st.session_state.target_url, c_id)
                
                for p in c['samples'][:40]:
                    # Calcul du score GEO avanc√©
                    score_data = calculate_page_score(p)
                    if isinstance(score_data, tuple):
                        sc, grade, breakdown, recommendations = score_data
                    else:
                        sc = score_data
                        grade = 'N/A'
                        breakdown = {}
                        recommendations = []
                    
                    # √âCHELLE DE COULEUR S√âV√àRE (vente : tout doit crier "probl√®me")
                    if expert_on:
                        if sc >= 95:
                            col = "#10b981"  # Vert - Parfait uniquement
                        elif sc >= 50:
                            col = "#f97316"  # Orange alerte - Non optimis√©
                        else:
                            col = "#ef4444"  # Rouge - Critique
                    else:
                        col = "#e2e8f0"  # Gris tr√®s clair standard
                    
                    # Label avec note si mode expert
                    label = get_clean_label(p.get('title',''), p['url'], domain)
                    if expert_on and isinstance(score_data, tuple):
                        label = f"{label}\\n[{grade}]"
                    
                    # TOOLTIP D√âTAILL√â avec √©l√©ments manquants
                    tooltip_parts = []
                    if expert_on and isinstance(score_data, tuple):
                        tooltip_parts.append(f"Score AI-READABLE: {sc}/100 - Grade: {grade}")
                        
                        # Ajouter les √©l√©ments manquants
                        missing = []
                        if not p.get('description'):
                            missing.append("‚ùå Meta description")
                        if not p.get('h1'):
                            missing.append("‚ùå H1")
                        if not p.get('has_structured_data'):
                            missing.append("‚ùå JSON-LD")
                        if p.get('h2_count', 0) < 2:
                            missing.append("‚ö†Ô∏è Peu de H2")
                        
                        if missing:
                            tooltip_parts.append("\\n\\n√âl√©ments manquants:\\n" + "\\n".join(missing))
                        
                        # Ajouter les recommandations principales
                        if recommendations:
                            top_reco = recommendations[:2]
                            tooltip_parts.append("\\n\\nRecommandations:\\n" + "\\n".join(f"‚Ä¢ {r}" for r in top_reco))
                    
                    tooltip = "\\n".join(tooltip_parts) if tooltip_parts else ""
                    
                    G.add_node(
                        p['url'], 
                        label=label, 
                        size=12, 
                        color=col,
                        font={'color': '#0f172a', 'face': 'Inter'},
                        title=tooltip
                    )
                    G.add_edge(c_id, p['url'])
            
            # 5. Rendu du graphe avec l√©gende si mode sant√©
            render_interactive_graph(G, show_health=expert_on)
    
    # ============= TAB 2 : M√âTHODOLOGIE =============
    with tab2:
        render_methodologie()
