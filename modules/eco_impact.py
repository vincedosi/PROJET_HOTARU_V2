# =============================================================================
# RSE & ECO-SCORE - HOTARU
# Calculatrice d'impact carbone : r√©duction des tokens = sobri√©t√© num√©rique
# =============================================================================

import re
import json

from bs4 import BeautifulSoup

from core.scraping import fetch_page

try:
    import tiktoken
    HAS_TIKTOKEN = True
except ImportError:
    HAS_TIKTOKEN = False

try:
    import trafilatura
    HAS_TRAFILATURA = True
except ImportError:
    HAS_TRAFILATURA = False


# Constantes physiques (ordre de grandeur)
# E_Token : √©nergie moyenne (kWh) pour traiter 1000 tokens en inf√©rence (NVIDIA H100)
KWH_PER_1K_TOKENS = 0.0004
# I_Carbone : intensit√© carbone moyenne mondiale (gCO2/kWh)
GCO2_PER_KWH = 475
# √âquivalence : 1 recharge smartphone ~ 0.012 kWh
KWH_PER_SMARTPHONE_CHARGE = 0.012


class EcoImpactCalculator:
    """
    Calcule l'√©conomie de tokens et l'impact carbone
    entre une page "brute" (HTML complet) et une page optimis√©e HOTARU (signal pur).
    """

    def __init__(self, timeout=15):
        self.timeout = timeout
        self._enc = None

    def _get_encoding(self):
        if not HAS_TIKTOKEN:
            return None
        if self._enc is None:
            self._enc = tiktoken.get_encoding("cl100k_base")
        return self._enc

    def _count_tokens(self, text: str) -> int:
        if not text or not HAS_TIKTOKEN:
            return 0
        enc = self._get_encoding()
        return len(enc.encode(text))

    def get_dirty_tokens(self, url: str) -> int:
        """
        R√©cup√®re le HTML complet, extrait tout le texte visible (menu, footer, etc.)
        et compte les tokens (poids cognitif "sale").
        """
        try:
            html = fetch_page(url, timeout=self.timeout)
        except Exception:
            return 0
        soup = BeautifulSoup(html, "html.parser")
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()
        text = soup.get_text(separator=" ", strip=True)
        text = re.sub(r"\s+", " ", text)
        return self._count_tokens(text)

    def _extract_jsonld(self, soup: BeautifulSoup) -> str:
        out = []
        for tag in soup.find_all("script", type="application/ld+json"):
            if tag.string:
                try:
                    data = json.loads(tag.string)
                    out.append(json.dumps(data, ensure_ascii=False))
                except (json.JSONDecodeError, TypeError):
                    pass
        return " ".join(out) if out else ""

    def _extract_main_content(self, html: str, url: str) -> str:
        if HAS_TRAFILATURA:
            try:
                main = trafilatura.extract(html, include_comments=False)
                if main and len(main.strip()) > 100:
                    return main.strip()
            except Exception:
                pass
        soup = BeautifulSoup(html, "html.parser")
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()
        main = soup.find("main") or soup.find("article") or soup.find("div", class_=re.compile(r"content|main|article", re.I))
        if main:
            return main.get_text(separator=" ", strip=True)
        return soup.get_text(separator=" ", strip=True)

    def get_clean_tokens(self, url: str) -> int:
        """
        Simule une extraction HOTARU : Titre + H1 + Contenu principal + JSON-LD.
        Si pas de JSON-LD, on simule ~10% de la taille du texte brut comme "structure".
        """
        try:
            html = fetch_page(url, timeout=self.timeout)
        except Exception:
            return 0
        soup = BeautifulSoup(html, "html.parser")
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()

        parts = []

        title = soup.find("title")
        if title and title.string:
            parts.append(title.get_text(strip=True))

        h1 = soup.find("h1")
        if h1:
            parts.append(h1.get_text(separator=" ", strip=True))

        main_text = self._extract_main_content(html, url)
        if main_text:
            parts.append(main_text)

        jsonld = self._extract_jsonld(soup)
        if jsonld:
            parts.append(jsonld)
        else:
            raw_len = len(" ".join(parts))
            if raw_len > 0:
                simulated_ld = " ".join(parts)[: max(100, raw_len // 10)]
                parts.append(simulated_ld)

        clean_text = " ".join(parts)
        clean_text = re.sub(r"\s+", " ", clean_text)
        return self._count_tokens(clean_text)

    def calculate(self, url: str) -> dict:
        """
        Calcule l'√©conomie : tokens, kWh, gCO2.
        Renvoie un dictionnaire avec toutes les m√©triques + donn√©es pour le graphique.
        """
        if not HAS_TIKTOKEN:
            return {
                "error": "tiktoken non install√©. Installez avec: pip install tiktoken",
                "tokens_dirty": 0,
                "tokens_clean": 0,
                "tokens_saved": 0,
                "kwh_saved": 0.0,
                "co2_saved": 0.0,
                "smartphone_charges": 0.0,
            }
        try:
            tokens_dirty = self.get_dirty_tokens(url)
            tokens_clean = self.get_clean_tokens(url)
        except Exception as e:
            return {
                "error": str(e),
                "tokens_dirty": 0,
                "tokens_clean": 0,
                "tokens_saved": 0,
                "kwh_saved": 0.0,
                "co2_saved": 0.0,
                "smartphone_charges": 0.0,
            }
        tokens_saved = max(0, tokens_dirty - tokens_clean)
        kwh_saved = (tokens_saved / 1000.0) * KWH_PER_1K_TOKENS
        co2_saved = kwh_saved * GCO2_PER_KWH
        smartphone_charges = kwh_saved / KWH_PER_SMARTPHONE_CHARGE if KWH_PER_SMARTPHONE_CHARGE else 0

        return {
            "error": None,
            "tokens_dirty": tokens_dirty,
            "tokens_clean": tokens_clean,
            "tokens_saved": tokens_saved,
            "kwh_saved": round(kwh_saved, 6),
            "co2_saved": round(co2_saved, 2),
            "smartphone_charges": round(smartphone_charges, 1),
        }


# =============================================================================
# UI STREAMLIT (Calculatrice + M√©thodologie)
# =============================================================================

def render_eco_tab():
    """Onglet principal RSE & Eco-Score : Calculatrice + M√©thodologie."""
    import streamlit as st

    st.markdown(
        "<p class='section-title'>üåç RSE & Eco-Score</p>",
        unsafe_allow_html=True,
    )
    st.markdown(
        "<p class='home-tagline' style='margin-bottom:1.5rem;'>"
        "Sobri√©t√© num√©rique ‚Äî Prouver que l'optimisation des donn√©es (JSON-LD, llms.txt) r√©duit la consommation d'√©nergie des IA.</p>",
        unsafe_allow_html=True,
    )

    tab_calc, tab_methodo = st.tabs(["Calculatrice d'Impact", "M√©thodologie Scientifique"])

    with tab_calc:
        _render_calculatrice()
    with tab_methodo:
        _render_methodologie()


def _render_calculatrice():
    import streamlit as st
    import plotly.graph_objects as go

    st.markdown("#### Calculatrice d'Impact Carbone")
    url = st.text_input(
        "URL du site web",
        placeholder="https://www.example.com",
        key="eco_impact_url",
    )
    if st.button("Simuler l'√©conomie Carbone", type="primary", use_container_width=True, key="eco_btn"):
        if not url or not url.strip():
            st.warning("Veuillez saisir une URL.")
            return
        with st.spinner("Analyse du poids cognitif en cours..."):
            calc = EcoImpactCalculator()
            result = calc.calculate(url.strip())

        if result.get("error"):
            st.error(result["error"])
            return

        st.markdown("---")
        st.markdown("##### M√©triques cl√©s")

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Tokens √©conomis√©s", f"{result['tokens_saved']:,}")
        with col2:
            st.metric("√ânergie √©pargn√©e (kWh)", f"{result['kwh_saved']:.6f}")
        with col3:
            st.metric("gCO‚ÇÇ √©vit√©", f"{result['co2_saved']:.2f}")
        with col4:
            st.metric("√âquivalence", f"~{result['smartphone_charges']:.0f} recharges smartphone")

        st.markdown("##### Comparatif : Poids cognitif")
        fig = go.Figure(
            data=[
                go.Bar(name="Poids cognitif actuel (brut)", x=["Tokens"], y=[result["tokens_dirty"]], marker_color="#e74c3c"),
                go.Bar(name="Poids cognitif HOTARU (optimis√©)", x=["Tokens"], y=[result["tokens_clean"]], marker_color="#27ae60"),
            ]
        )
        fig.update_layout(
            barmode="group",
            template="plotly_white",
            margin=dict(l=40, r=40, t=40, b=40),
            height=320,
            font=dict(size=12),
            legend=dict(orientation="h", yanchor="bottom", y=1.02),
            xaxis_title="",
            yaxis_title="Nombre de tokens",
        )
        st.plotly_chart(fig, use_container_width=True)

        st.markdown(
            f"<p style='font-size:0.9rem; color: var(--text-muted);'>"
            f"Ce gain √©quivaut √† environ <strong>{result['smartphone_charges']:.0f} recharges de smartphone</strong>.</p>",
            unsafe_allow_html=True,
        )


def _render_methodologie():
    import streamlit as st

    st.markdown("#### M√©thodologie Scientifique")
    st.markdown("---")

    st.markdown("""
## La Physique de la Donn√©e : Pourquoi structurer, c'est optimiser

### 1. Le Probl√®me : L'Ob√©sit√© Num√©rique des LLMs

Les mod√®les de langage (GPT-4, Claude, Gemini) ne ¬´ lisent ¬ª pas comme des humains. Ils **tokenisent** le code HTML brut avant de l'analyser.

**Un site web classique non optimis√© contient :**
- Des balises HTML complexes (nav, footer, aside)
- Des scripts JavaScript inline
- Du CSS embarqu√©
- Des menus, breadcrumbs, widgets

**Cons√©quence :** Pour extraire une information simple (prix, horaires, localisation), un LLM doit traiter **des milliers de tokens parasites**.

**Impact direct :**

*Plus de tokens = Plus de cycles GPU = Plus d'√©lectricit√© = Plus de CO‚ÇÇ*

---

### 2. La Solution HOTARU : Signal vs. Bruit

HOTARU structure l'information via :
- **JSON-LD** (donn√©es s√©mantiques au format Schema.org)
- **Extraction du contenu principal** (titre, H1, paragraphes cl√©s)
- **Suppression du bruit** (scripts, CSS, navigation)

**R√©sultat :** L'IA acc√®de directement au "signal pur" sans traiter le superflu.

---

### 3. La Formule d'Impact Carbone

Notre calculatrice mesure la **r√©duction de charge cognitive** des serveurs d'inf√©rence.

$$
\\Delta_{CO_2} = (Tokens_{Bruts} - Tokens_{Optimis√©s}) \\times E_{Token} \\times I_{Carbone}
$$

**Avec :**
- **Tokens** : Unit√© de lecture des LLMs (~0,75 mot)
- **E_Token** : √ânergie pour traiter 1000 tokens en inf√©rence (NVIDIA H100)
- **I_Carbone** : Intensit√© carbone du mix √©lectrique mondial

---

### 4. Ordres de Grandeur & Honn√™tet√© Scientifique

#### **Impact par requ√™te (estimation) :**
- Site non optimis√© : ~5000 tokens
- Site HOTARU : ~500 tokens
- **√âconomie : 4500 tokens**

**En CO‚ÇÇ :**
- 4500 tokens ‚âà 0,0001 kWh (H100)
- √ó 475g CO‚ÇÇ/kWh (mix mondial)
- = **~0,05g CO‚ÇÇ √©conomis√© par requ√™te**

#### **Pour √©conomiser 1 kg de CO‚ÇÇ :**
- Il faut **20 000 requ√™tes optimis√©es**
- √âquivalent √† **~250 emails** (4g CO‚ÇÇ/email)

#### **Donc oui, l'impact individuel est faible.**

**MAIS :**
- Si 10 000 entreprises optimisent leurs sites
- Et que chacune re√ßoit 1000 requ√™tes IA/jour
- **√âconomie annuelle : ~180 tonnes de CO‚ÇÇ**
- √âquivalent √† **~30 vols Paris-New York**

---

### 5. M√©thodologie & Limites

#### **Sources des coefficients :**
- **E_Token (0,0004 kWh/1000 tokens)** : Bas√© sur les benchmarks NVIDIA H100 pour l'inf√©rence GPT-4 (2023)
- **I_Carbone (475g CO‚ÇÇ/kWh)** : Mix √©lectrique mondial moyen (IEA 2024)
- **√âquivalence smartphone** : Batterie 3000 mAh = 0,012 kWh

#### **Ce que nous NE mesurons PAS :**
- L'empreinte carbone de **l'entra√Ænement** des mod√®les (90% de l'impact total de l'IA)
- Les variations selon le **data center** (France = 60g CO‚ÇÇ/kWh, Pologne = 700g)
- Les variations selon le **mod√®le** (GPT-4 ‚â† Claude ‚â† Gemini)

#### **Mise √† jour :**
Nous r√©visons nos coefficients **trimestriellement** selon les nouvelles publications scientifiques.

---

### Conclusion : Sobri√©t√© Num√©rique √† l'√âchelle

HOTARU ne va pas "sauver la plan√®te" √† lui seul.

**Mais :**
- Multipli√© par des milliers d'organisations
- Cet effet marginal devient un **standard de sobri√©t√©**
- Une infrastructure AI plus efficiente = Moins de serveurs n√©cessaires

**En structurant vos donn√©es, vous :**
1. Gagnez en visibilit√© IA (GEO)
2. R√©duisez la charge des data centers
3. Participez √† un √©cosyst√®me num√©rique plus sobre

---

#### üìä M√©thodologie compl√®te & sources
Documentation technique disponible sur demande.

#### üí¨ Questions ou corrections ?
Feedback scientifique bienvenu.
""")


def render_eco_tab_standalone():
    """Point d'entr√©e pour app.py."""
    return render_eco_tab()
