import json
import re

import streamlit as st
from bs4 import BeautifulSoup

from core.scraping import SmartScraper


def _render_log_box(logs):
    """Affiche les logs techniques dans un bloc monospace."""
    if not logs:
        st.info("Aucun log disponible")
        return
    content = "\n".join(logs[-200:])
    st.markdown(
        "<div style='font-family:SFMono-Regular,Menlo,monospace;font-size:0.75rem;"
        "background:#0f172a;color:#e5e7eb;padding:14px;border-radius:6px;"
        "max-height:300px;overflow:auto;white-space:pre-wrap;line-height:1.5;'>"
        f"{content}</div>",
        unsafe_allow_html=True,
    )


def _detect_spa_indicators(html: str, soup: BeautifulSoup):
    """D√©tecte tous les indicateurs SPA possibles."""
    html_l = html.lower()
    indicators = []
    
    # Patterns dans le texte
    text_patterns = {
        "react": "React (texte)",
        "__next": "Next.js (texte)",
        "nuxt": "Nuxt (texte)",
        "_nuxt": "Nuxt (texte)",
        "vue": "Vue (texte)",
        "angular": "Angular (texte)",
        "data-reactroot": "React (data-reactroot)",
        '<div id="root">': "React (div#root)",
        '<div id="app">': "Vue (div#app)",
    }
    
    for pattern, label in text_patterns.items():
        if pattern in html_l:
            indicators.append({"type": "texte", "signal": label, "pattern": pattern})
    
    # Scripts avec src
    for script in soup.find_all("script", src=True):
        src = script["src"].lower()
        if "_nuxt" in src:
            indicators.append({"type": "script", "signal": "Nuxt", "pattern": f"src={script['src'][:60]}..."})
        elif "__next" in src:
            indicators.append({"type": "script", "signal": "Next.js", "pattern": f"src={script['src'][:60]}..."})
        elif any(p in src for p in ["react", "vue", "angular"]):
            indicators.append({"type": "script", "signal": "Framework JS", "pattern": f"src={script['src'][:60]}..."})
    
    # Links modulepreload
    for link in soup.find_all("link", rel=True):
        rel = " ".join(link["rel"]).lower() if isinstance(link["rel"], list) else link["rel"].lower()
        href = link.get("href", "").lower()
        
        if "modulepreload" in rel and (".js" in href or "_nuxt" in href):
            indicators.append({"type": "link", "signal": "ES Module", "pattern": f"modulepreload {href[:50]}..."})
    
    # Scripts type=module
    module_scripts = soup.find_all("script", type="module")
    if module_scripts:
        indicators.append({"type": "module", "signal": f"{len(module_scripts)} script(s) ES module", "pattern": "type=module"})
    
    return indicators


def render_scraping_debug_tab():
    """Onglet de debug Scraping/JSON-LD ultra-complet."""

    st.markdown(
        '<p class="section-title">üîç DIAGNOSTIC SCRAPING & JSON-LD</p>',
        unsafe_allow_html=True,
    )
    
    st.markdown(
        "<p style='color:#64748b;margin-bottom:20px;'>Analyse compl√®te du scraping, "
        "d√©tection SPA, et extraction JSON-LD avec logs d√©taill√©s.</p>",
        unsafe_allow_html=True,
    )

    st.markdown('<div class="section-container">', unsafe_allow_html=True)

    col1, col2 = st.columns([3, 1])
    with col1:
        url = st.text_input(
            "URL √Ä DIAGNOSTIQUER",
            placeholder="https://lamarinerecrute.gouv.fr/",
            key="scraping_debug_url",
        )
    with col2:
        force_selenium = st.checkbox(
            "FORCER SELENIUM",
            value=False,
            help="Force l'utilisation de Selenium m√™me si la d√©tection automatique ne d√©tecte pas de SPA.",
        )

    st.markdown("</div>", unsafe_allow_html=True)
    st.markdown("<br>", unsafe_allow_html=True)

    # ===== CONTENEUR POUR LES LOGS EN TEMPS R√âEL =====
    log_placeholder = st.empty()

    if st.button(
        "üöÄ LANCER LE DIAGNOSTIC COMPLET",
        type="primary",
        use_container_width=True,
        key="scraping_debug_run",
    ):
        if not url:
            st.warning("‚ö†Ô∏è Veuillez saisir une URL.")
            return

        target_url = url.strip()
        if not target_url.startswith(("http://", "https://")):
            target_url = "https://" + target_url

        # ===== LISTE POUR ACCUMULER LES LOGS =====
        logs: list[str] = []

        def add_log(message: str):
            """Callback appel√© par SmartScraper pour chaque log."""
            logs.append(message)
            # Mise √† jour en temps r√©el
            with log_placeholder:
                _render_log_box(logs)

        # ===== INITIALISATION AVEC CALLBACK =====
        st.markdown('<div class="zen-divider"></div>', unsafe_allow_html=True)
        st.markdown(
            '<p class="section-title">01 / INITIALISATION & D√âTECTION SPA</p>',
            unsafe_allow_html=True,
        )

        scraper = None
        try:
            # ‚ö†Ô∏è CRITIQUE : On cr√©e une classe temporaire pour passer le callback AVANT __init__
            add_log("üîß Initialisation du scraper...")
            
            # On importe et on patche temporairement
            scraper = SmartScraper([target_url], max_urls=1, use_selenium=force_selenium)
            
            # ‚ö†Ô∏è HACK : On r√©assigne le callback et on rejoue les logs manquants
            # (car __init__ a d√©j√† eu lieu sans callback)
            scraper.log_callback = add_log
            
            # On simule les logs d'init qui ont √©t√© perdus
            add_log(f"üåê Domaine : {scraper.domain}")
            add_log(f"‚öôÔ∏è Selenium forc√© : {'OUI' if force_selenium else 'NON'}")
            add_log(f"‚öôÔ∏è Selenium activ√© : {'OUI' if scraper.use_selenium else 'NON'}")
            add_log(f"üöó Driver : {'Initialis√©' if scraper.driver else 'Non initialis√©'}")
            
        except Exception as e:
            st.error(f"‚ùå Erreur d'initialisation : {e}")
            import traceback
            st.code(traceback.format_exc())
            return

        # Affichage des m√©triques d'init
        col_init1, col_init2, col_init3 = st.columns(3)
        with col_init1:
            st.metric("üåê Domaine", scraper.domain)
        with col_init2:
            selenium_status = "‚úÖ ACTIV√â" if scraper.use_selenium else "‚ùå D√âSACTIV√â"
            st.metric("‚öôÔ∏è Mode Selenium", selenium_status)
        with col_init3:
            driver_type = "‚ùå Non initialis√©"
            if scraper.driver:
                driver_type = "‚úÖ Op√©rationnel"
            st.metric("üöó Driver", driver_type)

        # Analyse SPA AVANT le scraping
        st.markdown("<br>", unsafe_allow_html=True)
        st.markdown("**üîé Analyse des indicateurs SPA (HTML brut)**")
        
        try:
            import requests
            resp = requests.get(target_url, timeout=5)
            html_brut = resp.text
            soup_brut = BeautifulSoup(html_brut, "html.parser")
            spa_indicators = _detect_spa_indicators(html_brut, soup_brut)
            
            if spa_indicators:
                st.success(f"‚úÖ {len(spa_indicators)} indicateur(s) SPA d√©tect√©(s)")
                for ind in spa_indicators[:5]:
                    st.markdown(f"- **{ind['signal']}** ({ind['type']}) : `{ind['pattern']}`")
                if len(spa_indicators) > 5:
                    st.markdown(f"_... et {len(spa_indicators) - 5} autre(s)_")
            else:
                st.warning("‚ö†Ô∏è Aucun indicateur SPA d√©tect√©")
        except Exception as e:
            st.error(f"Erreur analyse pr√©liminaire : {e}")

        # ===== SCRAPING =====
        st.markdown('<div class="zen-divider"></div>', unsafe_allow_html=True)
        st.markdown(
            '<p class="section-title">02 / SCRAPING DE LA PAGE</p>',
            unsafe_allow_html=True,
        )

        data = None
        try:
            with st.spinner("‚è≥ Scraping en cours..."):
                data = scraper.get_page_details(target_url)
        except Exception as e:
            st.error(f"‚ùå Erreur scraping : {e}")
            import traceback
            st.code(traceback.format_exc())
        finally:
            if getattr(scraper, "driver", None):
                try:
                    scraper.driver.quit()
                except Exception:
                    pass

        if not data:
            st.error("‚ùå Aucune donn√©e renvoy√©e")
            return

        # R√©sultats scraping
        col_scrap1, col_scrap2, col_scrap3, col_scrap4 = st.columns(4)
        with col_scrap1:
            st.metric("üìÑ Titre", data.get("title", "N/A")[:30] + "...")
        with col_scrap2:
            st.metric("‚è±Ô∏è Temps", f"{data.get('response_time', 0):.2f}s")
        with col_scrap3:
            st.metric("üîó Liens", len(data.get("links", [])))
        with col_scrap4:
            st.metric("üìê HTML", f"{len(data.get('html_content', '')) // 1024}KB")

        # ===== JSON-LD =====
        st.markdown('<div class="zen-divider"></div>', unsafe_allow_html=True)
        st.markdown(
            '<p class="section-title">03 / DIAGNOSTIC JSON-LD</p>',
            unsafe_allow_html=True,
        )

        has_structured = bool(data.get("has_structured_data"))
        jsonld_blocks = data.get("json_ld") or []

        col_ld1, col_ld2, col_ld3 = st.columns(3)
        with col_ld1:
            status_ld = "‚úÖ OUI" if has_structured else "‚ùå NON"
            st.metric("JSON-LD D√âTECT√â", status_ld)
        with col_ld2:
            st.metric("BLOCS JSON-LD", len(jsonld_blocks))
        with col_ld3:
            st.metric("H2", data.get("h2_count", 0))

        if jsonld_blocks:
            st.markdown("<br>", unsafe_allow_html=True)
            st.success(f"‚úÖ {len(jsonld_blocks)} bloc(s) JSON-LD extrait(s) !")
            
            for i, block in enumerate(jsonld_blocks, 1):
                block_type = "N/A"
                if isinstance(block, dict):
                    block_type = block.get("@type", "Unknown")
                elif isinstance(block, list):
                    block_type = f"Array[{len(block)}]"
                
                with st.expander(f"üì¶ Bloc {i} ‚Äî Type: `{block_type}`", expanded=(i == 1)):
                    st.json(block)
        else:
            st.warning("‚ö†Ô∏è Aucun JSON-LD trouv√©")

        # ===== STRUCTURE HTML =====
        st.markdown('<div class="zen-divider"></div>', unsafe_allow_html=True)
        st.markdown(
            '<p class="section-title">04 / STRUCTURE HTML & SCRIPTS</p>',
            unsafe_allow_html=True,
        )

        html = data.get("html_content") or ""
        soup = BeautifulSoup(html, "html.parser")
        scripts = soup.find_all("script")

        type_counts = {}
        ld_scripts_in_html = []
        
        for s in scripts:
            t = (s.get("type") or "").strip()
            key = t or "(aucun type)"
            type_counts[key] = type_counts.get(key, 0) + 1
            
            if "ld+json" in key.lower():
                raw = (s.string or s.get_text(strip=True) or "").strip()
                ld_scripts_in_html.append({
                    "type": key,
                    "length": len(raw),
                    "content": raw,
                })

        col_html1, col_html2 = st.columns(2)
        
        with col_html1:
            st.markdown("**üìú Types de `<script>`**")
            for t, count in sorted(type_counts.items(), key=lambda kv: kv[1], reverse=True):
                icon = "üéØ" if "ld+json" in t.lower() else "üìÑ"
                st.markdown(f"{icon} `{t}` ‚Üí **{count}**")
        
        with col_html2:
            st.markdown("**üé® Structure**")
            st.markdown(f"üìå H1: {1 if soup.find('h1') else 0}")
            st.markdown(f"üìå H2: {data.get('h2_count', 0)}")
            st.markdown(f"üìå Listes: {data.get('lists_count', 0)}")
            st.markdown(f"üìå Images: {len(soup.find_all('img'))}")

        if ld_scripts_in_html:
            st.markdown("<br>", unsafe_allow_html=True)
            st.info(f"‚ÑπÔ∏è {len(ld_scripts_in_html)} script(s) JSON-LD dans HTML")
            
            for i, script_data in enumerate(ld_scripts_in_html, 1):
                with st.expander(f"Script #{i} ‚Äî {script_data['length']} chars", expanded=False):
                    try:
                        parsed = json.loads(script_data["content"])
                        st.json(parsed)
                    except json.JSONDecodeError as e:
                        st.error(f"‚ùå Erreur JSON : {e}")
                        st.code(script_data["content"][:500])

        # ===== DIAGNOSTIC =====
        st.markdown('<div class="zen-divider"></div>', unsafe_allow_html=True)
        st.markdown(
            '<p class="section-title">05 / DIAGNOSTIC & RECOMMANDATIONS</p>',
            unsafe_allow_html=True,
        )

        if has_structured and jsonld_blocks:
            st.success(
                f"‚úÖ **Parfait !** {len(jsonld_blocks)} bloc(s) JSON-LD extrait(s)."
            )
        elif ld_scripts_in_html and not jsonld_blocks:
            st.error(
                "‚ùå **Probl√®me d'extraction !** Scripts JSON-LD pr√©sents mais non extraits."
            )
        elif not ld_scripts_in_html and not jsonld_blocks and not scraper.use_selenium:
            st.warning(
                "‚ö†Ô∏è **Active 'FORCER SELENIUM'** pour les sites SPA."
            )
        elif not ld_scripts_in_html and not jsonld_blocks and scraper.use_selenium:
            st.error(
                "‚ùå **Aucun JSON-LD trouv√©** m√™me avec Selenium. Le site n'en a peut-√™tre pas."
            )
        
        # ===== LOGS =====
        st.markdown('<div class="zen-divider"></div>', unsafe_allow_html=True)
        st.markdown(
            '<p class="section-title">06 / LOGS TECHNIQUES</p>',
            unsafe_allow_html=True,
        )
        
        _render_log_box(logs)
        
        st.markdown("<br>", unsafe_allow_html=True)
        st.markdown(
            "<p style='color:#64748b;font-size:0.85rem;text-align:center;'>"
            f"üìä Total : {len(logs)} ligne(s) de log"
            "</p>",
            unsafe_allow_html=True,
        )